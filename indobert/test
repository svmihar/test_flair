python run_language_modeling.py --train_data_file /root/.train_lm/semua.txt --output_dir ./indoberto-small-v1 --model_type roberta --mlm --config_name ./berto --tokenizer_name ./berto --do_train --learning_rate 1e-4 --num_train_epochs 50 --save_total_limit 2 --save_steps 2000 --per_gpu_train_batch_size 16 --seed 42 --model_name_or_path ./indoberto-small-v1/checkpoint-76000 --overwrite_output_dir
